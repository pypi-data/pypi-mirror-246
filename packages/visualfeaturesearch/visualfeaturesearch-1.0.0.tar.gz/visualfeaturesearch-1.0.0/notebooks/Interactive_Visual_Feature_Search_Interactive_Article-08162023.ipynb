{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/lookingglasslab/VisualFeatureSearch/blob/main/notebooks/Interactive_Visual_Feature_Search_Interactive_Article.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WoJi893HA3q"
   },
   "source": [
    "# Interactive Visual Feature Search\n",
    "## [Devon Ulrich](https://github.com/devonulrich) and [Ruth Fong](http://ruthfong.com/), [Princeton University](https://princeton.edu/)\n",
    "\n",
    "This interactive article contains most qualitative visualizations from our [full paper](https://arxiv.org/pdf/2211.15060.pdf). For more details, please see the paper. Our code is also released [here](https://github.com/lookingglasslab/VisualFeatureSearch).\n",
    "\n",
    "We recommend you execute cell blocks sequentially one at a time. In particular, make sure to *download the ImageNet validation images* (see [the next cell](https://colab.research.google.com/drive/1D10Dsej-DAPzZawb5obxSwLQrIyod4g8#scrollTo=Cam0LxMfGS3I) for instructions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cam0LxMfGS3I"
   },
   "source": [
    "##  Download ImageNet validation images (**Action Required**)\n",
    "We use a subset of the ImageNet validation set as our searchable database for visual feature search. To get the dataset:\n",
    "1. Go to image-net.org and login or signup for access.\n",
    "2. Go to the following URL: https://image-net.org/challenges/LSVRC/2012/2012-downloads.php\n",
    "3. Copy the URL for downloading all validation images (with file size of 6.3Gb), and paste the URL below.\n",
    "4. Run the script below to download the validation set and the ImageNet devkit in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prznCGkIL-p1",
    "outputId": "62225176-b2ec-47fc-aeca-e8c23f46f97e"
   },
   "outputs": [],
   "source": [
    "!wget https://vissearch.blob.core.windows.net/data/ILSVRC2012_img_val.tar\n",
    "!wget https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRJr6tc8QPn-"
   },
   "source": [
    "## Other downloads and Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CqQiBStiMGz1",
    "outputId": "30191437-73b2-4946-b2a0-4ffae64cbfaf"
   },
   "outputs": [],
   "source": [
    "!wget https://azcopyvnext.azureedge.net/release20220511/azcopy_linux_amd64_10.15.0.tar.gz\n",
    "!tar -xvf azcopy_linux_amd64_10.15.0.tar.gz\n",
    "\n",
    "!git clone https://github.com/lookingglasslab/VisualFeatureSearch.git\n",
    "!pip install zarr\n",
    "\n",
    "# unofficial lucid port to pytorch\n",
    "!pip install git+https://github.com/greentfrapp/lucent\n",
    "\n",
    "!./azcopy_linux_amd64_10.15.0/azcopy copy https://vissearch.blob.core.windows.net/data/ResNet_ImageNet_val.tar.gz ResNet_ImageNet_val.tar.gz\n",
    "!tar -xzf ResNet_ImageNet_val.tar.gz\n",
    "\n",
    "!wget https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vzp2jj7s8x7n",
    "outputId": "be120efe-89b9-4a8b-fe76-c2e59951e4d8"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "import ast\n",
    "from textwrap import wrap\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "from lucent.optvis import render\n",
    "import zarr\n",
    "\n",
    "sys.path.append('./VisualFeatureSearch')\n",
    "from vissearch import widgets, util, data\n",
    "from vissearch.searchtool import CachedSearchTool, get_crop_rect\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "else:\n",
    "  raise Exception('No GPU available')\n",
    "\n",
    "sv_inet_model = torchvision.models.resnet50(pretrained=True)\n",
    "sv_inet_model = sv_inet_model.cuda().eval()\n",
    "\n",
    "def sv_inet_conv5(X):\n",
    "    hook = render.ModuleHook(sv_inet_model.layer4[2].conv2)\n",
    "    sv_inet_model(X)\n",
    "    hook.close()\n",
    "    return hook.features\n",
    "\n",
    "CONV5_FEATURE_SIZE = 7 # row/column length for the layer of interest\n",
    "\n",
    "# set up dataset\n",
    "imagenet_dataset = torchvision.datasets.ImageNet('./', 'val', transform=data.net_transform)\n",
    "vis_imagenet_dataset = torchvision.datasets.ImageNet('./', 'val', transform=data.vis_transform)\n",
    "!rm ILSVRC2012_img_val.tar\n",
    "\n",
    "with open('imagenet1000_clsidx_to_labels.txt', 'r') as f:\n",
    "  imagenet_labels = ast.literal_eval(f.read())\n",
    "\n",
    "# set up cached search tool\n",
    "sv_inet_store = zarr.DirectoryStore('ResNet_ImageNet_val')\n",
    "sv_inet_root = zarr.group(store=sv_inet_store, overwrite=False)\n",
    "sv_inet_conv5_data = sv_inet_root['conv5']\n",
    "\n",
    "cached_search_tool = CachedSearchTool(sv_inet_conv5, sv_inet_conv5_data, device, batch_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Nb6ldeQILag"
   },
   "source": [
    "## Motivation\n",
    "\n",
    "In the past decade, the field of interpretability has focused on developing a variety of tools for understanding the behavior of convolutional neural networks (CNNs). Much of this work has been on developing *static* visualizations that elucidate an aspect of a model. For instance, *attribution heatmaps* aim to highlight the image regions that are most responsible for a model's output decision, while *feature visualizations* aim to visualize an internal component of a model (e.g. the patterns that most activate a specific CNN channel).\n",
    "\n",
    "However, static visualizations do not utilize the full expressive power of modern web technologies. We argue that well-designed *interactive visualizations* are useful for allowing machine learning (ML) researchers to quickly easily explore models.\n",
    "Currently, most interactive visualizations are specific to a limited set of models (i.e. they are more like to well-polished demos) but are not easily extendable to novel models. This aspect limits their ability to be used by researchers in their regular workflow for understanding their own, custom models.\n",
    "\n",
    "## Visual Feature Search\n",
    "*This section corresponds to Section 3 in our full paper.*\n",
    "\n",
    "We introduce a novel interactive visualization tool, Visual Feature Search, that allows users to easily and quickly perform a \"reverse search\" using CNN features.\n",
    "\n",
    "The main steps of our tool are as follows:\n",
    "\n",
    "1. A user selects the model, layer, and dataset they want to explore.\n",
    "2. A user highlights a free-form region in an query image using our interactive highlighting widget.\n",
    "2. Our tool searches through a datset of images for other image regions that have the most similar CNN features to the highlighted region.\n",
    "3. Our tool retrieves and shows the most similar image regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDimCik9L_mI"
   },
   "source": [
    "## Basic Example\n",
    "*This example corresponds with Figures 1 and 2 in our full paper.*\n",
    "\n",
    "First, select the image you'd like to visualize and then hold down your mouse click and highlight the region you'd like to search for similarly encoded regions.\n",
    "\n",
    "To reproduce Fig. 1, select the first dog image and highlight its face. To reproduce Fig. 2, select the second church image and highlight its tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "f12szYPSMeRq",
    "outputId": "6ec47e9e-d006-4a67-870f-886b5639eea5"
   },
   "outputs": [],
   "source": [
    "# download ten query images to use\n",
    "query_img_ids = [\"004\", \"530\", \"495\", \"211\", \"652\", \"021\", \"686\", \"016\", \"713\", \"198\"]\n",
    "\n",
    "def load_img(idx: str):\n",
    "  url = \"https://vissearch.blob.core.windows.net/query-imgs/ILSVRC2012_test_00000\" + idx + \".JPEG\"\n",
    "  img = Image.open(requests.get(url, stream=True).raw).convert('RGB')\n",
    "  return img\n",
    "\n",
    "query_model_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "query_imgs = [load_img(id) for id in query_img_ids]\n",
    "vis_query_imgs = [data.vis_transform(img) for img in query_imgs]\n",
    "model_query_imgs = [data.net_transform(img) for img in query_imgs]\n",
    "\n",
    "# convert images to Data URLs so we can pass them into the HTML widget\n",
    "query_img_urls = [util.image_to_durl(img) for img in vis_query_imgs]\n",
    "\n",
    "highlight_data = None\n",
    "highlight_index = None\n",
    "def highlight_callback(data):\n",
    "    global highlight_data, highlight_index\n",
    "    highlight_data = data[0]\n",
    "    highlight_index = data[1]\n",
    "util.create_callback('highlight_callback', highlight_callback)\n",
    "\n",
    "widgets.MultiHighlightWidget(all_urls=query_img_urls, callback_name='highlight_callback')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QQHJ4pvPdWh"
   },
   "source": [
    "Next, we search for the most similar image regions, as encoded by an ImageNet-trained ResNet50 in the 5th convolutional block (see \"Experimental Details\" for more on layer selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rrtx2u97otBC",
    "outputId": "c5c73fbe-b17b-4794-f9a3-418337c7dde6"
   },
   "outputs": [],
   "source": [
    "assert highlight_data is not None, \"Use the widget to highlight an image region\"\n",
    "\n",
    "selected_idx = int(highlight_index)\n",
    "mask = util.durl_to_image(highlight_data)\n",
    "\n",
    "cached_search_tool.set_input_image(model_query_imgs[selected_idx])\n",
    "\n",
    "conv5_transform = transforms.Resize(7)\n",
    "conv5_mask = conv5_transform(mask)\n",
    "conv5_mask_arr = np.asarray(conv5_mask)[:,:,3] / 255\n",
    "\n",
    "print('Loading results...')\n",
    "conv5_sims, conv5_xs, conv5_ys = cached_search_tool.compute(conv5_mask_arr)\n",
    "conv5_order = torch.argsort(conv5_sims, descending=True)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrQqCQPQPFd3"
   },
   "source": [
    "Now, we show the top 5 most similar image regions and show their cosine similarity score to the highlighted region in the query image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "-_1ftRqippY1",
    "outputId": "2176e861-0116-413d-fba0-d7d797361735"
   },
   "outputs": [],
   "source": [
    "# visualize results\n",
    "# set up the figure\n",
    "DISPLAY_NUM = 5\n",
    "\n",
    "fig = plt.figure(figsize=(11.6, 5.15), dpi=100)\n",
    "fig.suptitle('RN50 conv5 nearest neighbors', fontweight='bold', y=0.75)\n",
    "gs = fig.add_gridspec(1,2, width_ratios=[1,5])\n",
    "results_gs = gs[1].subgridspec(1,DISPLAY_NUM)\n",
    "\n",
    "# show the query region on the left-hand side\n",
    "ax = fig.add_subplot(gs[0])\n",
    "ax.axis('off')\n",
    "ax.imshow(util.mask_overlay(vis_query_imgs[selected_idx], 0, 0, 224, np.asarray(mask)[:,:,3] / 256))\n",
    "ax.add_line(matplotlib.lines.Line2D([285,285], [0,224], lw=2, color='black')).set_clip_on(False)\n",
    "\n",
    "for i in range(DISPLAY_NUM):\n",
    "  idx = conv5_order[i]\n",
    "  curr_img_out = util.mask_overlay(vis_imagenet_dataset[idx][0],\n",
    "                                   x=conv5_xs[idx],\n",
    "                                   y=conv5_ys[idx],\n",
    "                                   mask_size=7,\n",
    "                                   mask=util.crop_mask(conv5_mask_arr))\n",
    "\n",
    "  ax = fig.add_subplot(results_gs[i])\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  ax.imshow(curr_img_out, cmap='gray')\n",
    "  ax.set_title( f'{conv5_sims[idx].cpu().numpy():.02f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35nu07hbTnFy"
   },
   "source": [
    "## Experimental details\n",
    "\n",
    "Unless otherwise specified, we visualize **ResNet50 models**.\n",
    "Typically, works study the features immediately after a major convolutional block.\n",
    "However, we study the features from the **second to last convolutional layer** within these blocks.\n",
    "\n",
    "For instance, instead of extracting features from conv4, block 6, layer 3 (i.e. `layer3.5.2` in [torchvision](https://github.com/pytorch/vision)'s ResNet50), we extract features from conv4, block 6, layer 2 (i.e. `layer3.5.1`).\n",
    "\n",
    "This is because features after the conv4 block are 4x larger than the features after the penultimate layer in the conv4 block (i.e. storing uncompressed, conv4-block features for 50k images require $\\approx$ 70GB), and we choose to make our experiments more portable and available on low-memory environments like Google Colab.\n",
    "\n",
    "For brevity, we refer to the features after the second convolutional in each residual block by their \"convX\" group (i.e. \"conv4\" in this notebook refers to conv4, block 6, layer 2, a.k.a. `layer3.5.1` in torchvision's ResNet50)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oIlFnAmMfTj"
   },
   "source": [
    "## In- vs. Out-of-Domain Images\n",
    "*This section corresponds to Section 4.1 in our full paper and reproduces rows a-c in Figure 3.*\n",
    "\n",
    "One potential use of our tool is to understand how robust a model is when presented with novel images. In this example, we visualize a ResNet50 model trained on ImageNet for several different query images:\n",
    "1. In-domain images from the ImageNet test set\n",
    "2. Out-of-domain images from ImageNet-A ([Hendricks et al., CVPR 2021](https://github.com/hendrycks/natural-adv-examples))\n",
    "3. Out-of-domain images from ImageNet-Sketch ([Wang et al., NeurIPS 2019](https://github.com/HaohanWang/ImageNet-Sketch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pP_KIeASqC0"
   },
   "source": [
    "First, download the ImageNet-A and ImageNet-Sketch datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VfsbIxCiOX_4",
    "outputId": "5ac08404-1577-47c4-c453-a810f2092e8f"
   },
   "outputs": [],
   "source": [
    "# Download ImageNet-A\n",
    "!wget https://people.eecs.berkeley.edu/~hendrycks/imagenet-a.tar\n",
    "!tar -xf imagenet-a.tar\n",
    "!rm imagenet-a/README.txt\n",
    "\n",
    "# Download ImageNet-Sketch\n",
    "!wget https://huggingface.co/datasets/imagenet_sketch/resolve/main/data/ImageNet-Sketch.zip\n",
    "!unzip ImageNet-Sketch.zip\n",
    "\n",
    "!rm ImageNet-Sketch.zip\n",
    "!rm imagenet-a.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2AFPIYzSxLw"
   },
   "source": [
    "Next, we show the three query images with the same ImageNet class (i.e. images of mosques) from each of the respective datasets (ImageNet, ImageNet-A, and ImageNet-Sketch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "ykeXI_NFOcSa",
    "outputId": "d71835ad-83e8-4cc0-c83d-0ac69d5d1098"
   },
   "outputs": [],
   "source": [
    "inet_path = 'val/n03788195/ILSVRC2012_val_00026188.JPEG'\n",
    "inet_a_path = 'imagenet-a/n03788195/0.000251_stingray _ stingray_0.7409649.jpg'\n",
    "sketch_path = 'sketch/n03788195/sketch_26.JPEG'\n",
    "\n",
    "inet_img = Image.open(inet_path).convert('RGB')\n",
    "inet_a_img = Image.open(inet_a_path).convert('RGB')\n",
    "sketch_img = Image.open(sketch_path).convert('RGB')\n",
    "\n",
    "vis_inet_img = data.vis_transform(inet_img)\n",
    "vis_inet_a_img = data.vis_transform(inet_a_img)\n",
    "vis_sketch_img = data.vis_transform(sketch_img)\n",
    "\n",
    "cnn_inet_img = data.net_transform(inet_img)\n",
    "cnn_inet_a_img = data.net_transform(inet_a_img)\n",
    "cnn_sketch_img = data.net_transform(sketch_img)\n",
    "\n",
    "# visualize the three images\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(vis_inet_img, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('ImageNet')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(vis_inet_a_img, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('ImageNet-A')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(vis_sketch_img, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('ImageNet-Sketch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WXetf_rS8C5"
   },
   "source": [
    "Then, we highlight the same canonical part in each query image (i.e. the dome on a mosque).\n",
    "\n",
    "*See Fig. 3 for query image regions to reproduce the figure.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "I4XDRRXGP0hZ",
    "outputId": "3c72c079-28d8-4d25-9622-a1ef268deb38"
   },
   "outputs": [],
   "source": [
    "inet_callback_data = None\n",
    "def inet_highlight(data):\n",
    "    global inet_callback_data\n",
    "    inet_callback_data = data\n",
    "util.create_callback('inet_highlight', inet_highlight)\n",
    "\n",
    "print('ImageNet Mosque:')\n",
    "widgets.HighlightWidget(util.image_to_durl(vis_inet_img), callback_name='inet_highlight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "5n7oucOmP1OT",
    "outputId": "bb858d5c-679c-4a4f-e2c8-689e0dffce8b"
   },
   "outputs": [],
   "source": [
    "inet_a_callback_data = None\n",
    "def inet_a_highlight(data):\n",
    "    global inet_a_callback_data\n",
    "    inet_a_callback_data = data\n",
    "util.create_callback('inet_a_highlight', inet_a_highlight)\n",
    "\n",
    "print(\"ImageNet-A Mosque:\")\n",
    "widgets.HighlightWidget(util.image_to_durl(vis_inet_a_img), callback_name='inet_a_highlight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "gtm0KMFJP26o",
    "outputId": "91d87d97-ed19-4658-c581-42431d9b9392"
   },
   "outputs": [],
   "source": [
    "sketch_callback_data = None\n",
    "def sketch_highlight(data):\n",
    "    global sketch_callback_data\n",
    "    sketch_callback_data = data\n",
    "util.create_callback('sketch_highlight', sketch_highlight)\n",
    "\n",
    "print(\"ImageNet-Sketch Mosque:\")\n",
    "widgets.HighlightWidget(util.image_to_durl(vis_sketch_img), callback_name='sketch_highlight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW7b4YjoTQSs"
   },
   "source": [
    "We then search for the most similar regions as encoded in a ResNet50 conv5 features for each of the highlighted regions from the query images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_PCEz-GP4nn",
    "outputId": "1b6956f1-5f90-4fca-b444-63f3873553e3"
   },
   "outputs": [],
   "source": [
    "assert inet_callback_data is not None, \"1st widget needs a highlighted region\"\n",
    "assert inet_a_callback_data is not None, \"2nd widget needs a highlighted region\"\n",
    "assert sketch_callback_data is not None, \"3rd widget needs a highlighted region\"\n",
    "\n",
    "conv5_transform = transforms.Resize(CONV5_FEATURE_SIZE)\n",
    "\n",
    "# assemble masks\n",
    "inet_mask = util.durl_to_image(inet_callback_data)\n",
    "inet_mask_small = conv5_transform(inet_mask)\n",
    "inet_mask_arr = np.asarray(inet_mask_small)[:,:,3] / 255\n",
    "\n",
    "inet_a_mask = util.durl_to_image(inet_a_callback_data)\n",
    "inet_a_mask_small = conv5_transform(inet_a_mask)\n",
    "inet_a_mask_arr = np.asarray(inet_a_mask_small)[:,:,3] / 255\n",
    "\n",
    "sketch_mask = util.durl_to_image(sketch_callback_data)\n",
    "sketch_mask_small = conv5_transform(sketch_mask)\n",
    "sketch_mask_arr = np.asarray(sketch_mask_small)[:,:,3] / 255\n",
    "\n",
    "# compute the similarities\n",
    "print('Loading Results...')\n",
    "\n",
    "# ImageNet\n",
    "cached_search_tool.set_input_image(cnn_inet_img)\n",
    "inet_sims, inet_xs, inet_ys = cached_search_tool.compute(inet_mask_arr)\n",
    "inet_order = torch.argsort(inet_sims, descending=True)\n",
    "\n",
    "# ImageNet-A\n",
    "cached_search_tool.set_input_image(cnn_inet_a_img)\n",
    "inet_a_sims, inet_a_xs, inet_a_ys = cached_search_tool.compute(inet_a_mask_arr)\n",
    "inet_a_order = torch.argsort(inet_a_sims, descending=True)\n",
    "\n",
    "# ImageNet-Sketch\n",
    "cached_search_tool.set_input_image(cnn_sketch_img)\n",
    "sketch_sims, sketch_xs, sketch_ys = cached_search_tool.compute(sketch_mask_arr)\n",
    "sketch_order = torch.argsort(sketch_sims, descending=True)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z773Rz30Tata"
   },
   "source": [
    "Finally, we show the most similar regions to each of the 3 query images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "zhdXWEVGP8PF",
    "outputId": "00f8c88f-6767-42c5-f55b-1ec79dced32f"
   },
   "outputs": [],
   "source": [
    "# set up the figure\n",
    "DISPLAY_NUM = 5\n",
    "TEXT_CUTOFF = 6\n",
    "\n",
    "toTensor = transforms.ToTensor()\n",
    "\n",
    "fig = plt.figure(figsize=(11.6, 5.15), dpi=100)\n",
    "gs = fig.add_gridspec(1,2, width_ratios=[1,5])\n",
    "queries_gs = gs[0].subgridspec(3,1)\n",
    "results_gs = gs[1].subgridspec(3,DISPLAY_NUM)\n",
    "\n",
    "# show the query region on the left-hand side\n",
    "ax = fig.add_subplot(queries_gs[0])\n",
    "ax.axis('off')\n",
    "ax.imshow(util.mask_overlay(vis_inet_img, 0, 0, 224, np.asarray(inet_mask)[:,:,3] / 256))\n",
    "ax.text(-40, 124, 'a', weight='bold', size=24, horizontalalignment='center')\n",
    "\n",
    "ax = fig.add_subplot(queries_gs[1])\n",
    "ax.axis('off')\n",
    "ax.imshow(util.mask_overlay(vis_inet_a_img, 0, 0, 224, np.asarray(inet_a_mask)[:,:,3] / 256))\n",
    "ax.text(-40, 124, 'b', weight='bold', size=24, horizontalalignment='center')\n",
    "ax.add_line(matplotlib.lines.Line2D([295,295], [-267,224+267], lw=2, color='black')).set_clip_on(False)\n",
    "\n",
    "ax = fig.add_subplot(queries_gs[2])\n",
    "ax.axis('off')\n",
    "ax.imshow(util.mask_overlay(vis_sketch_img, 0, 0, 224, np.asarray(sketch_mask)[:,:,3] / 256))\n",
    "ax.text(-40, 124, 'c', weight='bold', size=24, horizontalalignment='center')\n",
    "\n",
    "for i in range(DISPLAY_NUM):\n",
    "  # imagenet\n",
    "  idx = inet_order[i+1] # ignore first result, which will be the query image\n",
    "  curr_img_out = util.mask_overlay(vis_imagenet_dataset[idx][0],\n",
    "                                   inet_xs[idx],\n",
    "                                   inet_ys[idx],\n",
    "                                   7,\n",
    "                                   util.crop_mask(inet_mask_arr))\n",
    "\n",
    "  ax = fig.add_subplot(results_gs[0,i])\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  ax.imshow(curr_img_out, cmap='gray') # use cmap in case image is only 1-channel\n",
    "\n",
    "  gt_label = imagenet_labels[int(idx) // 50].split(',')[0]\n",
    "  if len(gt_label) > TEXT_CUTOFF+3:\n",
    "    gt_label = gt_label[:TEXT_CUTOFF] + '...'\n",
    "  sim_str = str(np.round(inet_sims[idx].cpu().numpy(), 2))\n",
    "  new_title = gt_label + ', ' + sim_str\n",
    "  ax.set_title(new_title, fontsize=12, pad=2)\n",
    "  if i==0:\n",
    "    ax.set_ylabel('IN', fontsize=16)\n",
    "\n",
    "  # imagenet-a\n",
    "  idx = inet_a_order[i]\n",
    "  curr_img_out = util.mask_overlay(vis_imagenet_dataset[idx][0],\n",
    "                                   inet_a_xs[idx],\n",
    "                                   inet_a_ys[idx],\n",
    "                                   7,\n",
    "                                   util.crop_mask(inet_a_mask_arr))\n",
    "\n",
    "  ax = fig.add_subplot(results_gs[1,i])\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  ax.imshow(curr_img_out, cmap='gray') # use cmap in case image is only 1-channel\n",
    "\n",
    "  gt_label = imagenet_labels[int(idx) // 50].split(',')[0]\n",
    "  if len(gt_label) > TEXT_CUTOFF+3:\n",
    "    gt_label = gt_label[:TEXT_CUTOFF] + '...'\n",
    "  sim_str = str(np.round(inet_a_sims[idx].cpu().numpy(), 2))\n",
    "  new_title = gt_label + ', ' + sim_str\n",
    "  ax.set_title(new_title, fontsize=12, pad=2)\n",
    "  if i==0:\n",
    "    ax.set_ylabel('IN-A', fontsize=16)\n",
    "\n",
    "  # imagenet-sketch\n",
    "  idx = sketch_order[i]\n",
    "  curr_img_out = util.mask_overlay(vis_imagenet_dataset[idx][0],\n",
    "                                   sketch_xs[idx],\n",
    "                                   sketch_ys[idx],\n",
    "                                   7,\n",
    "                                   util.crop_mask(sketch_mask_arr))\n",
    "\n",
    "\n",
    "  ax = fig.add_subplot(results_gs[2,i])\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  ax.imshow(curr_img_out, cmap='gray') # use cmap in case image is only 1-channel\n",
    "\n",
    "  gt_label = imagenet_labels[int(idx) // 50].split(',')[0]\n",
    "  if len(gt_label) > TEXT_CUTOFF+3:\n",
    "    gt_label = gt_label[:TEXT_CUTOFF] + '...'\n",
    "  sim_str = str(np.round(sketch_sims[idx].cpu().numpy(), 2))\n",
    "  new_title = gt_label + ', ' + sim_str\n",
    "  ax.set_title(new_title, fontsize=12, pad=2)\n",
    "  if i==0:\n",
    "    ax.set_ylabel('IN-S', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLzCZj5z44ph"
   },
   "source": [
    "The visualization above shows that the out-of-domain images (rows **b** and **c**) have lower cosine similarity scores than those of the in-domain images (row **a**), and they often possess a different ground-truth label than that of the query image. For instance, notice how the mosque ImageNet-Sketch results (row **c**) shows nearest neighbors that are other illustrations, rather than other mosque buildings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXFJlVhaOXRy"
   },
   "source": [
    "## Supervised vs. Self-Supervised Models\n",
    "*This section corresponds to Section 4.3 in our full paper and reproduces row a in Figure 4.*\n",
    "\n",
    "Another use case of our tool is to understand how different training paradigms influence a model's feature representation.\n",
    "For instance, much work in the past decade has focused on self-supervised learning, which aims to learn useful, general features without labels (in contrast to supervised learning, which requires labels for training).\n",
    "These approaches have steadily competed and now (as of 2020) surpass performance of supervised, ImageNet-trained models on certain tasks, when extremely large datasets of images are used for self-supervised training.\n",
    "\n",
    "In this example, we visualize representations of ResNet50 models trained on ImageNet using SimCLR self-supervision ([Chen et al., ICML 2020](https://github.com/google-research/simclr)) vs. standard full-supervision. In particular, we show results for the following three layers:\n",
    "1. SimCLR conv5\n",
    "2. Supervised conv4\n",
    "3. Supervised conv5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBygGfknySBi"
   },
   "source": [
    "We first download the activation caches for SimCLR- and ImageNet-trained ResNet50 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0U28MrzHUjVR",
    "outputId": "f1ae2df5-1727-454b-c232-da07c7fff632"
   },
   "outputs": [],
   "source": [
    "# free up space for new cache files\n",
    "!rm -rf sketch\n",
    "!rm -rf imagenet-a\n",
    "!rm -rf ResNet_ImageNet_val\n",
    "\n",
    "!./azcopy_linux_amd64_10.15.0/azcopy copy \"https://vissearch.blob.core.windows.net/data/ResNet_IN_6_20.tar.gz\" \"ResNet_IN_6_20.tar.gz\"\n",
    "!tar -xzf ResNet_IN_6_20.tar.gz\n",
    "!rm ResNet_IN_6_20.tar.gz\n",
    "\n",
    "!./azcopy_linux_amd64_10.15.0/azcopy copy \"https://vissearch.blob.core.windows.net/data/SimCLR_IN_6_20.tar.gz\" \"SimCLR_IN_6_20.tar.gz\"\n",
    "!tar -xzf SimCLR_IN_6_20.tar.gz\n",
    "!rm SimCLR_IN_6_20.tar.gz\n",
    "\n",
    "!pip install lightning-bolts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNfLCbsoyd3b"
   },
   "source": [
    "Then, we set-up our search tool to explore SimCLR conv5 and supervised conv4 and conv5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cea2G-7UanxO",
    "outputId": "edd5ef0f-8ca8-40d6-95bd-37847d1b6822"
   },
   "outputs": [],
   "source": [
    "from pl_bolts.models.self_supervised import SimCLR\n",
    "\n",
    "simclr_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset_simclr = torchvision.datasets.ImageNet('./', 'val', transform=simclr_transform)\n",
    "\n",
    "weight_path = 'https://pl-bolts-weights.s3.us-east-2.amazonaws.com/simclr/bolts_simclr_imagenet/simclr_imagenet.ckpt'\n",
    "simclr_model = SimCLR.load_from_checkpoint(weight_path, strict=False)\n",
    "simclr_model = simclr_model.encoder\n",
    "simclr_model = simclr_model.cuda().eval()\n",
    "\n",
    "def simclr_inet_conv5(X):\n",
    "    hook = render.ModuleHook(simclr_model.layer4[2].conv2)\n",
    "    simclr_model(X)\n",
    "    hook.close()\n",
    "    return hook.features\n",
    "\n",
    "def sv_inet_conv4(X):\n",
    "    hook = render.ModuleHook(sv_inet_model.layer3[5].conv2)\n",
    "    sv_inet_model(X)\n",
    "    hook.close()\n",
    "    return hook.features\n",
    "\n",
    "supervised_store = zarr.DirectoryStore('ResNet_IN_6_20')\n",
    "supervised_root = zarr.group(store=supervised_store, overwrite=False)\n",
    "sv_conv4_arr = supervised_root['conv4']\n",
    "sv_conv5_arr = supervised_root['conv5']\n",
    "\n",
    "sv_conv4_st = CachedSearchTool(sv_inet_conv4, sv_conv4_arr, device, batch_size=5000)\n",
    "sv_conv5_st = CachedSearchTool(sv_inet_conv5, sv_conv5_arr, device, batch_size=5000)\n",
    "\n",
    "simclr_store = zarr.DirectoryStore('SimCLR_IN_6_20')\n",
    "simclr_root = zarr.group(store=simclr_store, overwrite=False)\n",
    "simclr_arr = simclr_root['conv5']\n",
    "\n",
    "simclr_conv5_st = CachedSearchTool(simclr_inet_conv5, simclr_arr, device, batch_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3827OOmyngp"
   },
   "source": [
    "Next, we select a region in the image. To reproduce Fig. 4 row a, select the entire neck of the bird (see Fig. 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "U121B4q8b5e8",
    "outputId": "3dccc6fc-954b-458c-bb81-f8dd29ed596d"
   },
   "outputs": [],
   "source": [
    "url = 'https://vissearch.blob.core.windows.net/query-imgs/ILSVRC2012_test_00000652.JPEG'\n",
    "query_img = Image.open(requests.get(url, stream=True).raw).convert('RGB')\n",
    "vis_query_img = data.vis_transform(query_img)\n",
    "simclr_query_img = simclr_transform(query_img)\n",
    "sv_query_img = data.net_transform(query_img)\n",
    "\n",
    "# convert images to Data URLs so we can pass them into the HTML widget\n",
    "query_img_urls = [util.image_to_durl(img) for img in vis_query_imgs]\n",
    "query_url = util.image_to_durl(vis_query_img)\n",
    "\n",
    "highlight_data = None\n",
    "def highlight_callback(data):\n",
    "    global highlight_data\n",
    "    highlight_data = data\n",
    "util.create_callback('highlight_callback', highlight_callback)\n",
    "\n",
    "widgets.HighlightWidget(img_url=query_url, callback_name='highlight_callback')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kr31XQN6y8Ho"
   },
   "source": [
    "We then search through activations for the nearest neighbors in SimCLR conv5, supervised conv5, and supervised conv4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "diEe9dxSfM4y",
    "outputId": "863d47a0-220a-4294-ca94-aa5ebd6c859a"
   },
   "outputs": [],
   "source": [
    "assert highlight_data is not None, \"Use the widget to highlight an image region\"\n",
    "\n",
    "mask = util.durl_to_image(highlight_data)\n",
    "\n",
    "sv_conv4_st.set_input_image(sv_query_img)\n",
    "sv_conv5_st.set_input_image(sv_query_img)\n",
    "simclr_conv5_st.set_input_image(simclr_query_img)\n",
    "\n",
    "conv4_transform = transforms.Resize(14)\n",
    "conv4_mask = conv4_transform(mask)\n",
    "conv4_mask_arr = np.asarray(conv4_mask)[:,:,3] / 255\n",
    "\n",
    "conv5_transform = transforms.Resize(7)\n",
    "conv5_mask = conv5_transform(mask)\n",
    "conv5_mask_arr = np.asarray(conv5_mask)[:,:,3] / 255\n",
    "\n",
    "print('Loading results...')\n",
    "sv4_sims, sv4_xs, sv4_ys = sv_conv4_st.compute(conv4_mask_arr)\n",
    "sv5_sims, sv5_xs, sv5_ys = sv_conv5_st.compute(conv5_mask_arr)\n",
    "clr_sims, clr_xs, clr_ys = simclr_conv5_st.compute(conv5_mask_arr)\n",
    "\n",
    "sv4_order = torch.argsort(sv4_sims, descending=True)\n",
    "sv5_order = torch.argsort(sv5_sims, descending=True)\n",
    "clr_order = torch.argsort(clr_sims, descending=True)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-Rl9l8szF4t"
   },
   "source": [
    "Finally, we show the top-5 nearest neighbors for each of the layers (CLR-5 denotes SimCLR conv5, while SV-4 and SV-5 denote supervised conv4 and supervised conv5 respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "8rmWPPL3fX4L",
    "outputId": "e7a7940f-74fc-4407-d914-90e168ff3332"
   },
   "outputs": [],
   "source": [
    "# visualize results\n",
    "DISPLAY_NUM = 5\n",
    "\n",
    "fig = plt.figure(figsize=(11.6, 5.15), dpi=100)\n",
    "gs = fig.add_gridspec(1,2, width_ratios=[1,5])\n",
    "results_gs = gs[1].subgridspec(3,DISPLAY_NUM)\n",
    "\n",
    "# show the query region on the left-hand side\n",
    "ax = fig.add_subplot(gs[0])\n",
    "ax.axis('off')\n",
    "ax.imshow(util.mask_overlay(vis_query_img, 0, 0, 224, np.asarray(mask)[:,:,3] / 256))\n",
    "ax.add_line(matplotlib.lines.Line2D([285,285], [-210,224+210], lw=2, color='black')).set_clip_on(False)\n",
    "\n",
    "\n",
    "for i in range(DISPLAY_NUM):\n",
    "  # handle SimCLR features\n",
    "  idx = clr_order[i]\n",
    "  curr_img_out = util.mask_overlay(dataset_simclr[idx][0].permute(1,2,0) * 256,\n",
    "                                   x=clr_xs[idx],\n",
    "                                   y=clr_ys[idx],\n",
    "                                   mask_size=7,\n",
    "                                   mask=util.crop_mask(conv5_mask_arr))\n",
    "\n",
    "  ax = fig.add_subplot(results_gs[0,i])\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  ax.imshow(curr_img_out, cmap='gray') # use cmap in case image is only 1-channel\n",
    "  ax.set_title( f'{clr_sims[idx].cpu().numpy():.02f}', pad=0)\n",
    "  if i == 0:\n",
    "   ax.set_ylabel('CLR-5', fontsize=16)\n",
    "\n",
    "  # now do the same for sv-conv4 features\n",
    "  idx = sv4_order[i]\n",
    "  curr_img_out = util.mask_overlay(dataset_simclr[idx][0].permute(1,2,0) * 256,\n",
    "                                   x=sv4_xs[idx],\n",
    "                                   y=sv4_ys[idx],\n",
    "                                   mask_size=14,\n",
    "                                   mask=util.crop_mask(conv4_mask_arr))\n",
    "\n",
    "  ax = fig.add_subplot(results_gs[1,i])\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  ax.imshow(curr_img_out, cmap='gray')\n",
    "  ax.set_title( f'{sv4_sims[idx].cpu().numpy():.02f}', pad=0)\n",
    "  if i == 0:\n",
    "   ax.set_ylabel('SV-4', fontsize=16)\n",
    "\n",
    "  # finally display sv-conv5 features\n",
    "  idx = sv5_order[i]\n",
    "  curr_img_out = util.mask_overlay(dataset_simclr[idx][0].permute(1,2,0) * 256,\n",
    "                                   x=sv5_xs[idx],\n",
    "                                   y=sv5_ys[idx],\n",
    "                                   mask_size=7,\n",
    "                                   mask=util.crop_mask(conv5_mask_arr))\n",
    "\n",
    "  ax = fig.add_subplot(results_gs[2,i])\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  ax.imshow(curr_img_out, cmap='gray')\n",
    "  ax.set_title( f'{sv5_sims[idx].cpu().numpy():.02f}', pad=0)\n",
    "  if i == 0:\n",
    "   ax.set_ylabel('SV-5', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHnYZT6q4iRg"
   },
   "source": [
    "If Fig. 5, row a, was reproduced, notice how SimCLR conv5's nearest neighbors highlighted the necks of other bird species, while supervised conv5 tended to highlight other birds from the same species, and supervised conv4 appears to show results in between the two other layers.\n",
    "\n",
    "Qualitative visualizations like this one led us to believe that SimCLR conv4 was more representationally similar to supervised conv4 than supervised conv5. Thus, we quantitatively analyzed the similarities between the three layers in our full paper and found that, actually, SimCLR conv5 was more similar to supervised conv5 than supervised conv4 (see full paper for more details).\n",
    "This highlights an important limitation of our work, namely, that qualitative visualizations can help someone develop hypotheses about a model's behavior but that any hypotheses must be quantitatively validated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xUKqSdIUk4k"
   },
   "source": [
    "## ImageNet vs. PASS Pretraining\n",
    "\n",
    "*This section corresponds to Section 4.4 in our full paper and reproduces row a in Figure 7.*\n",
    "\n",
    "A third use case of our tool is to study how the choice of training dataset affects a model's features.\n",
    "To mitigate privacy concerns of training on images with humans, the PASS dataset ([Asano et al., NeurIPS Datasets 2021](https://www.robots.ox.ac.uk/~vgg/data/pass/)) was recently introduced as an ImageNet replacement for self-supervised learning.\n",
    "PASS is a large dataset of unlabelled images that does not contain human faces or body parts and has been shown to perform as well as ImageNet-trained models on human-centric tasks (i.e. pose estimation).\n",
    "\n",
    "In this example, we visualize two ResNet50 models trained via MOCO-v2 self-supervision using two different training datasets:\n",
    "1. PASS\n",
    "2. ImageNet\n",
    "\n",
    "Because we are particularly interested in how the two models compare in their ability to represent faces, we visualize results for query images containing human faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPgmEecZzWMZ"
   },
   "source": [
    "First, we download new activation cache files for PASS- and ImageNet-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L9ViibIIWC80",
    "outputId": "55d98b49-568a-4199-af0c-b35f87cba570"
   },
   "outputs": [],
   "source": [
    "# free up space for new cache file\n",
    "!rm -rf ResNet_IN_6_20\n",
    "!rm -rf SimCLR_IN_6_20\n",
    "\n",
    "!wget https://dl.fbaipublicfiles.com/moco/moco_checkpoints/moco_v2_800ep/moco_v2_800ep_pretrain.pth.tar\n",
    "\n",
    "# feature caches\n",
    "!./azcopy_linux_amd64_10.15.0/azcopy copy \"https://vissearch.blob.core.windows.net/data/PASS_MoCo_6_20.tar.gz\" \"PASS_MoCo_6_20.tar.gz\"\n",
    "!tar -xzf PASS_MoCo_6_20.tar.gz\n",
    "!./azcopy_linux_amd64_10.15.0/azcopy copy \"https://vissearch.blob.core.windows.net/data/ImageNet_MoCo_6_20.tar.gz\" \"ImageNet_MoCo_6_20.tar.gz\"\n",
    "!tar -xzf ImageNet_MoCo_6_20.tar.gz\n",
    "\n",
    "!rm PASS_MoCo_6_20.tar.gz\n",
    "!rm ImageNet_MoCo_6_20.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPkiMWLvz1wR"
   },
   "source": [
    "Then, we set up our search tool to search through activations in the two models (e.g. PASS- and ImageNet-trained ResNet50 models trained via MOCO-v2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lCTMDJUQj6Qj",
    "outputId": "378c7ede-95db-445d-9282-8d9fd608947b"
   },
   "outputs": [],
   "source": [
    "imagenet_moco = torchvision.models.resnet50()\n",
    "checkpoint = torch.load('moco_v2_800ep_pretrain.pth.tar')\n",
    "state_dict = checkpoint['state_dict']\n",
    "for k in list(state_dict.keys()):\n",
    "    # retain only encoder_q up to before the embedding layer\n",
    "    if k.startswith('module.encoder_q') and not k.startswith('module.encoder_q.fc'):\n",
    "        # remove prefix\n",
    "        state_dict[k[len(\"module.encoder_q.\"):]] = state_dict[k]\n",
    "    # delete renamed or unused k\n",
    "    del state_dict[k]\n",
    "\n",
    "msg = imagenet_moco.load_state_dict(state_dict, strict=False)\n",
    "assert set(msg.missing_keys) == {\"fc.weight\", \"fc.bias\"}\n",
    "\n",
    "imagenet_moco = imagenet_moco.eval().cuda()\n",
    "\n",
    "# load PASS MoCo\n",
    "pass_moco = torch.hub.load('yukimasano/PASS:main', 'moco_resnet50')\n",
    "pass_moco = pass_moco.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZaM44Q-XkCnN"
   },
   "outputs": [],
   "source": [
    "# model dissection methods\n",
    "\n",
    "def moco_inet_conv5(X):\n",
    "    hook = render.ModuleHook(imagenet_moco.layer4[2].conv2)\n",
    "    imagenet_moco(X)\n",
    "    hook.close()\n",
    "    return hook.features\n",
    "\n",
    "def moco_pass_conv5(X):\n",
    "    hook = render.ModuleHook(pass_moco.layer4[2].conv2)\n",
    "    pass_moco(X)\n",
    "    hook.close()\n",
    "    return hook.features\n",
    "\n",
    "inet_store = zarr.DirectoryStore('ImageNet_MoCo_6_20')\n",
    "supervised_root = zarr.group(store=inet_store, overwrite=False)\n",
    "inet_conv5_arr = supervised_root['conv5']\n",
    "\n",
    "inet_moco_st = CachedSearchTool(moco_inet_conv5, inet_conv5_arr, device, batch_size=5000)\n",
    "\n",
    "pass_store = zarr.DirectoryStore('PASS_MoCo_6_20')\n",
    "supervised_root = zarr.group(store=pass_store, overwrite=False)\n",
    "pass_conv5_arr = supervised_root['conv5']\n",
    "\n",
    "pass_moco_st = CachedSearchTool(moco_pass_conv5, pass_conv5_arr, device, batch_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6PYQkmL0Vgr"
   },
   "source": [
    "Next, we select a region of interest in an image. To reproduce Fig. 7 row a, select the entire face (see Fig. 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "bb4VNI1KkRan",
    "outputId": "3a318aee-7de1-4798-89ca-6fe765c4f6d0"
   },
   "outputs": [],
   "source": [
    "url = \"https://vissearch.blob.core.windows.net/query-imgs/ILSVRC2012_test_00000277.JPEG\"\n",
    "query_img = Image.open(requests.get(url, stream=True).raw).convert('RGB')\n",
    "\n",
    "vis_query_img = data.vis_transform(query_img)\n",
    "cnn_query_img = data.net_transform(query_img)\n",
    "\n",
    "# convert images to Data URLs so we can pass them into the HTML widget\n",
    "query_img_url = util.image_to_durl(vis_query_img)\n",
    "\n",
    "highlight_data = None\n",
    "def highlight_callback(data):\n",
    "    global highlight_data\n",
    "    highlight_data = data\n",
    "util.create_callback('highlight_callback', highlight_callback)\n",
    "\n",
    "widgets.HighlightWidget(img_url=query_img_url, callback_name='highlight_callback')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRiQvWdz0kjJ"
   },
   "source": [
    "We then search through the activation caches for the nearest neighbor image regions to the selected query region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JRR92orjkq9B",
    "outputId": "b7a0679f-b66f-4c52-b7fd-e2c050a180dd"
   },
   "outputs": [],
   "source": [
    "assert highlight_data is not None, \"Use the widget to highlight an image region\"\n",
    "\n",
    "mask = util.durl_to_image(highlight_data)\n",
    "\n",
    "inet_moco_st.set_input_image(cnn_query_img)\n",
    "pass_moco_st.set_input_image(cnn_query_img)\n",
    "\n",
    "conv5_transform = transforms.Resize(7)\n",
    "conv5_mask = conv5_transform(mask)\n",
    "conv5_mask_arr = np.asarray(conv5_mask)[:,:,3] / 255\n",
    "\n",
    "print('Loading results...')\n",
    "inet_sims, inet_xs, inet_ys = inet_moco_st.compute(conv5_mask_arr)\n",
    "pass_sims, pass_xs, pass_ys = pass_moco_st.compute(conv5_mask_arr)\n",
    "\n",
    "inet_order = torch.argsort(inet_sims, descending=True)\n",
    "pass_order = torch.argsort(pass_sims, descending=True)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLMXUdyO0wKp"
   },
   "source": [
    "Finally, we show the top-5 nearest neighbors (with cosine similarity scores) for each of the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "899Om5fokuzg",
    "outputId": "82bec55f-2576-4997-c773-7225e0074f1c"
   },
   "outputs": [],
   "source": [
    "# visualize results\n",
    "# set up the figure\n",
    "DISPLAY_NUM = 5\n",
    "\n",
    "fig = plt.figure(figsize=(11.6, 3.33), dpi=100)\n",
    "gs = fig.add_gridspec(1,2, width_ratios=[1,5])\n",
    "results_gs = gs[1].subgridspec(2,DISPLAY_NUM)\n",
    "\n",
    "# show the query region on the left-hand side\n",
    "ax = fig.add_subplot(gs[0])\n",
    "ax.axis('off')\n",
    "ax.imshow(util.mask_overlay(vis_query_img, 0, 0, 224, np.asarray(mask)[:,:,3] / 256))\n",
    "ax.add_line(matplotlib.lines.Line2D([270,270], [-100,224+100], lw=2, color='black')).set_clip_on(False)\n",
    "\n",
    "for i in range(DISPLAY_NUM):\n",
    "  # handle ImageNet features\n",
    "  idx = inet_order[i]\n",
    "  curr_img_out = util.mask_overlay(vis_imagenet_dataset[idx][0],\n",
    "                                   x=inet_xs[idx],\n",
    "                                   y=inet_ys[idx],\n",
    "                                   mask_size=7,\n",
    "                                   mask=util.crop_mask(conv5_mask_arr))\n",
    "\n",
    "  ax = fig.add_subplot(results_gs[0, i])\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  ax.imshow(curr_img_out, cmap='gray') # use cmap in case image is only 1-channel\n",
    "  ax.set_title(f'{inet_sims[idx].cpu().numpy():.2f}', fontsize=12, pad=0)\n",
    "  if i==0:\n",
    "    ax.set_ylabel('IN', fontsize=16)\n",
    "\n",
    "  # now do the same for PASS features\n",
    "  idx = pass_order[i]\n",
    "  curr_img_out = util.mask_overlay(vis_imagenet_dataset[idx][0],\n",
    "                                   x=pass_xs[idx],\n",
    "                                   y=pass_ys[idx],\n",
    "                                   mask_size=7,\n",
    "                                   mask=util.crop_mask(conv5_mask_arr))\n",
    "\n",
    "  ax = fig.add_subplot(results_gs[1, i])\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  ax.imshow(curr_img_out, cmap='gray')\n",
    "  ax.set_title(f'{pass_sims[idx].cpu().numpy():.2f}', fontsize=12, pad=0)\n",
    "  if i==0:\n",
    "    ax.set_ylabel('PASS', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U24oV0M34yRj"
   },
   "source": [
    "If the face was selected as the query region (as in Fig. 7, row a), notice how both models' nearest neighbors are also other faces (despite the fact that the PASS-trained model never saw human faces during training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwEHUEeD1L6T"
   },
   "source": [
    "## Conclusion\n",
    "In this notebook, we highlight how to use our visual feature search tool to understand what are the nearest neighbors in activation space for a model. In particular, we highlight three use cases for our tool:\n",
    "1. understanding how in- vs. **out-of-domain images** are represented by a model\n",
    "2. comparing representations between fully-supervised and **self-supervised models**\n",
    "3. analyzing the effect that the choice of the training dataset (specifically **PASS vs. ImageNet**) has on a model's feature representation.\n",
    "\n",
    "In our full paper, we also show results from several quantitative experiments that paired with some of our qualitative visualizations as well as explored a fourth use case of our tool: to visualize how a representation has changed after a model has been edited. Please see our full paper for more details on those experiments and visualizations not shown in this notebook.\n",
    "\n",
    "#### Limitations\n",
    "The main limitation of our work is its qualitative nature: While our tool is useful for developing intuition about how a model works; any hypotheses should be substantiated with quantitative experiments.\n",
    "We caution users from using our tool alone to confirm hypotheses and suggest that it be used as an exploratory tool in conjunction with other visualizations and experiments (as done in Secs. 4.3 and 4.4 in our full paper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z19wvznTNa3f"
   },
   "source": [
    "## Citations\n",
    "For full citations and a complete set of references, please see our full paper.\n",
    "\n",
    "* Asano et al., NeurIPS Datasets 2021. PASS: An ImageNet replacement for self-supervised pretraining without humans. [[link](https://www.robots.ox.ac.uk/~vgg/data/pass/)]\n",
    "* Bau et al., SIGGRAPH 2019. Semantic photo manipulation with a generative image prior. [[link](https://ganpaint.io/)]\n",
    "* Chen et al., ICML 2020. A Simple Framework for Contrastive Learning of Visual Representations. [[link](https://github.com/google-research/simclr)]\n",
    "* Hendrycks et al., CVPR 2021. Natural adversarial examples. [[link](https://github.com/hendrycks/natural-adv-examples)]\n",
    "* Wang et al., NeurIPS 2019. Learning robust global representations by penalizing local predictive power. [[link](https://github.com/HaohanWang/ImageNet-Sketch)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZMdELGHW6oF"
   },
   "source": [
    "## Acknowledgements\n",
    "\n",
    "We are grateful for support from [Open Philanthropy](https://www.openphilanthropy.org/) (RF), the [Princeton Engineering Project X Fund](https://aspire-report.princeton.edu/engineering/project-x-fund) (RF), and [Princeton SEAS IW Funding](https://engineering.princeton.edu/undergraduate-studies/sophomore-senior-advising/senior-thesis-independent-work-funding) (DU).\n",
    "We thank [David Bau](https://baulab.info/), [Sunnie S. Y. Kim](https://sunniesuhyoung.github.io/), and Indu Panigrahi for helpful discussions and/or feedback on our tool.\n",
    "We also thank the authors of GANPaint ([Bau et al., SIGGRAPH 2019](https://ganpaint.io/)), whose [widget](https://github.com/CSAILVision/gandissect/blob/master/client/ganpaint.html) we based our highlighting widget on."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
