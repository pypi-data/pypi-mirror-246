import time

import redis as pyredis
import ujson as json
from devapp.app import app
from devapp.tools import FLG, define_flags, get_deep
from operators.con.connections import con_params
from operators.dec_enc import decode_msg, encode_msg, from_flat_json, msg_key
from operators.kv_tools import kv
from operators.build import stop
#         v = data
#         for p in pth:
#             v = v.get(p)
#             if v == None:
#                 app.warn('Key not found', pth=pth)
#                 return
#         return v


# def get_con_params(cls):
#     d = con_params(cls, use_dflt=True)
#     breakpoint()  # FIXME BREAKPOINT
#     p = {
#         'db': d.get('db', 0),
#         'host': d['hostname'],
#         'port': d.get('port', 6379),
#     }
#     cls.host_short = h = p['host']
#     if h in {'localhost', '127.0.0.1'}:
#         cls.host_short = 'L'

#     if d.get('password'):
#         p['password'] = d['password']
#     return p


def set_host_params(cls, p):
    cls.port = p['port']
    _ = {'localhost', '127.0.0.1'}
    cls.host_short = 'L' if p['host'] in _ else p['host']


class redis:
    name = 'redis'
    multi_url = True

    class con_defaults:
        # fmt:off
        _strip_non_default_keys  = True
        _cast                    = True
        _use                     = True
        charset                  = None
        client_name              = None
        db                       = 0
        decode_responses         = False
        encoding                 = 'utf-8'
        encoding_errors          = 'strict'
        errors                   = None
        health_check_interval    = 5
        host                     = '127.0.0.1'
        max_connections          = None
        password                 = None
        port                     = 6379
        retry_on_timeout         = True
        single_connection_client = False
        socket_connect_timeout   = None
        socket_keepalive         = None
        socket_keepalive_options = None
        socket_timeout           = 5
        ssl                      = False
        ssl_ca_certs             = None
        ssl_cert_reqs            = 'required'
        ssl_certfile             = None
        ssl_check_hostname       = False
        ssl_keyfile              = None
        unix_socket_path         = None
        username                 = None
        # fmt:on

    _con_ = None
    _pool_ = None
    host_short = None
    streams = None
    started_reading = False

    @classmethod
    def _con(cls):
        # we return a standing connection, not pool managed, since this is 2.5x slower, for fast get/set
        if cls._con_ is None:
            p = cls._con_params = con_params(cls, defaults=cls.con_defaults)
            set_host_params(cls, p[0])
            if len(p) == 1:
                cls._con_ = pyredis.Redis(**p[0])
            else:
                C = pyredis.cluster
                h, port, password = p[0]['host'], p[0]['port'], p[0].get('password')
                N = C.ClusterNode
                nodes = [N(i.get('host', h), port=i.get('port', port)) for i in p]
                cls._con_ = C.RedisCluster(startup_nodes=nodes, password=password)
        return cls._con_

    @classmethod
    def _pool(cls):
        """we return a pool mangaged conn"""
        # return pyredis.Redis(host, port)
        return cls._con()  # has a con pool internally

        # if cls._pool_ is None:
        #     p = con_params(cls, dflt_keys=True)
        #     breakpoint()  # FIXME BREAKPOINT
        #     p = connection_pool = pyredis.ConnectionPool(**p)
        #     cls._pool_ = pyredis.Redis(connection_pool=p)
        # breakpoint()  # FIXME BREAKPOINT
        # return cls._pool_

    @classmethod
    def monitor(cls, observer):
        """Observing whats going on on redis

        Note: In cluster mode you'll see only the transactions on your configured
        startup nodes.
        """
        con = cls._con()
        with con.monitor() as m:
            for command in m.listen():
                observer.on_next(command)

    @classmethod
    def src(cls, observer, name='ax_stream', enc=None, key=None):
        """
        xread from a redis stream generated by us, i.e. messages with headers
        """
        cls.add_stream_reader(observer, name, enc, key)

    @classmethod
    def add_stream_reader(cls, observer, name, enc, key=None):
        """xread supports reading multiple streams
        => we concentrate all stream reads here and just call it once for all, then push to the observers

        what we do in this method is kick off the xread intervals and add readers into the
        read spec - which looks like:

        {<(stream) name 1>: last_read_item, <sn2>: ...}

        In params we keep the refs to observers for the streams

        """

        name = name.encode('utf-8')
        key = msg_key if key is None else key.encode('utf-8')

        if cls.streams is None:
            cls.streams = [{}, {}]
        spec, params = cls.streams
        if name in params:
            params[name]['observers'].append(observer)
        else:
            params[name] = {
                'observers': [observer],
                'enc': enc,
                'key': key,
            }
        # forget history for stream(s) called `name`, get from NOW
        spec[name] = '$'  # now the reader will start (at next interval), including us
        cls.start_reading()

    @classmethod
    def start_reading(cls):
        """called only once per con class, for all streams"""
        if cls.started_reading:
            return
        cls.started_reading = True
        con = cls._pool()

        def stopping(cls=cls):
            cls.started_reading = False

        host, port = cls.host_short, cls.port

        d = {'name': 'redis - %s' % cls.host_short, 'func': stopping}
        stop.append(d)
        # we are at process start, first stream to be read from that con class

        # we block half the socket timeout. When there is an event it returns immediate
        # This has to be anyway on a seperate greenlet
        try:
            timeout = cls._con_params[0]['socket_timeout']
        except Exception:
            timeout = 2
        spec, params = cls.streams

        blockms = max(100, int(timeout * 1000 / 2))
        while cls.started_reading:
            # r e.g. l[[b'mystreamname', [(b'1688545684957-0', {b'_msg_': b'myid'})]]] (a raw one)
            r = con.xread(spec, None, blockms)
            for n in r:
                name, recs = n[0], n[1]
                if not recs:
                    continue  # often the case, when we have many streams in spec
                nfos = params[name]
                key = nfos['key']
                enc = nfos['enc']
                obs = nfos['observers']
                for rec in recs:
                    val = rec[1][key]
                    if enc == 'raw':  # speed, e.g. for ipc id streams
                        [o.on_next(val) for o in obs]
                    else:
                        msg = decode_msg(val)
                        ids = msg.get('_ids')
                        if ids:
                            ids['strm'] = [name.decode('utf-8'), host, port]
                        [o.on_next(msg) for o in obs]
                    time.sleep(0)
                spec[name] = recs[-1][0]
            r = con.xread(spec, None, blockms)

    @classmethod
    def snk(cls, data, msg, name='ax_stream', enc=None):
        """
        Simple full message forwarder.
        In order to push dedicated payloads, snk to set_events.
        """
        con = cls._con()
        if enc == 'plain':
            m = data  # TODO: wtf, why data and not msg? use case?
        else:
            m, is_bin = encode_msg(msg, enc=enc)
        con.xadd(name, {msg_key: m})

    @classmethod
    def get(
        cls,
        data,
        msg,
        key='{d[id]}',
        pth=None,
        pth_sep='.',
        create=True,
        head=False,
        enc='auto',
    ):
        """
        Reads data from redis, then adds it into the data structure.
        lookup may be deep key within data
        {
        """
        keyv = key.format(d=data, m=msg)
        con = cls._con()
        v = con.get(keyv)
        if not v:
            return data if pth is None else v

        v = decode_msg(v)

        if pth is None:
            return v
        return kv.update(
            data, msg, pth=pth, pth_sep=pth_sep, create=create, head=head, **v
        )

    @classmethod
    def get_events(
        cls,
        data,
        msg,
        key='events:{d[id]}',
        pth=None,
        pth_sep='.',
        create=True,
        head=False,
        count=None,
        read_from='-',
        discard_from=0,
    ):
        """
        xrevrange, i.e. querying a stream as a time series db
        pth: None -> returns the events
             True -> inplace add under top level key 'events'
             'foo.bar' -> inplace add under ['foo']['bar']]['events']
        read_from: read from this exact ts, or, dflt, from the oldest one avail
        discard_from: read but discard all older than this (in ms or in "-2000" => since 2 secs
        """
        keyv = key.format(d=data, m=msg)
        con = cls._con()
        evts = con.xrevrange(keyv, count=count, min=read_from)
        r = []
        if discard_from:
            if isinstance(discard_from, str):
                if discard_from[0] == '-':
                    discard_from = (time.time() * 1000) - int(discard_from[1:])
                else:
                    discard_from = int(discard_from)
            evts = [e for e in evts if int(e[0][:13]) >= discard_from]
        for e in evts:
            ts, m = e
            # fixme - cannot deserialize string values:
            m = from_flat_json(m)
            r.append([int(ts[:13]), m])
        if pth is None:
            return r
        if pth is True:
            # add under top level key "events"
            path = ''
        kv.update(data, msg, pth, create=create, head=head, events=r)

    @classmethod
    def set(
        cls,
        data,
        msg,
        key='{d[id]}',
        pth=None,
        pth_sep='.',
        create=False,
        head=False,
        ex=3600,
        px=None,
        nx=False,
        xx=False,
        keepttl=False,
        enc='json',
        _no_write=False,
        **kw,
    ):
        """
        sig compat with ax:set regarding common keys
        """
        if px:
            ex = None
        # key='{d[id]}',
        keyv = key.format(d=data, m=msg)
        kw = kw if _no_write is False else _no_write
        if kw:
            kv.update(data, msg, pth=pth, pth_sep=pth_sep, create=create, head=head, **kw)

        v = get_deep(key=pth, data=data, sep=pth_sep, create=create)

        con = cls._con()

        v = encode_msg(v, enc=enc)[0]

        if _no_write is not False:
            return con, keyv, v

        con.set(keyv, v, ex=ex, px=px, nx=nx, xx=xx, keepttl=keepttl)

    @classmethod
    def set_events(
        cls,
        data,
        msg,
        key='events:{d[id]}',
        pth=None,
        pth_sep='.',
        create=False,
        head=False,
        maxlen=10,
        ignore=None,
        enc='json_flat',
        _pipeline=False,
        **kw,
    ):
        """Redis streams write"""
        if isinstance(data, list) and msg.get('is_aggr') and '{d' in key:
            l = dict(locals())
            l.pop('cls')
            # we get a batch which we have to write one by one, since key is per data
            # -> go pipeline
            pipe = cls._con().pipeline()
            l['_pipeline'] = pipe
            for d in data:
                l['data'], l['msg'] = d['payload'], d
                cls.set_events(**l)
            pipe.execute()
            return

        if ignore:
            # just one top level key currently:
            ign = {}
            for i in ignore:
                ign[i] = data.pop(i, None)

        con, keyv, v = cls.set(
            data,
            msg,
            key,
            pth,
            pth_sep,
            create,
            head,
            enc=enc,
            _no_write=kw,
        )
        # res is only the timestamp:
        if _pipeline:
            con = _pipeline
        # API is strange, reuireds dict:
        if not isinstance(v, dict):
            v = {msg_key: v}
        res = con.xadd(name=keyv, fields=v, maxlen=maxlen, approximate=True)
        if ignore:
            data.update(ign)
