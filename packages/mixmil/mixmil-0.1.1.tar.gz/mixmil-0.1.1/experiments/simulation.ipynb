{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from mixmil import MixMIL\n",
    "from mixmil.data import load_data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_metrics(u_pred, w_pred, u, w):\n",
    "    rho_bag = st.spearmanr(u_pred, u).correlation  # bag level correlation\n",
    "    is_top_instance = (w > np.quantile(w, 0.90)).long().ravel()\n",
    "    auc_instance = roc_auc_score(is_top_instance, w_pred)  # instance-retrieval AUC\n",
    "    return rho_bag, auc_instance\n",
    "\n",
    "\n",
    "def calc_metrics(model, X, u, w):\n",
    "    u_pred = model.predict(X[\"test\"]).cpu().numpy()\n",
    "    w_pred = model.get_weights(X[\"test\"])[0].cpu().numpy()\n",
    "\n",
    "    P = u_pred.shape[1]\n",
    "    if P > 0:\n",
    "        rho_bag, auc_instance = [], []\n",
    "        for i in range(P):\n",
    "            _rho_bag, _auc_instance = _calc_metrics(\n",
    "                u_pred[..., i], w_pred[..., i].ravel(), u[\"test\"][..., i], w[\"test\"][..., i].ravel()\n",
    "            )\n",
    "            rho_bag.append(_rho_bag)\n",
    "            auc_instance.append(_auc_instance)\n",
    "\n",
    "        res_dict = {\n",
    "            \"rho_bag\": np.mean(rho_bag),\n",
    "            \"rho_bag_err\": np.std(rho_bag) / np.sqrt(P),\n",
    "            \"auc_instance\": np.mean(auc_instance),\n",
    "            \"auc_instance_err\": np.std(auc_instance) / np.sqrt(P),\n",
    "        }\n",
    "    else:\n",
    "        rho_bag, auc_instance = _calc_metrics(u_pred, w_pred.ravel(), u[\"test\"], w[\"test\"].ravel())\n",
    "        res_dict = {\"rho_bag\": rho_bag, \"auc_instance\": auc_instance}\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(prefix, metrics):\n",
    "    print(f\"{prefix} metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23b2a79444348288359e79609a4d2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GLMM Init:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] metrics:\n",
      "rho_bag: 0.6004\n",
      "rho_bag_err: 0.0131\n",
      "auc_instance: 0.5000\n",
      "auc_instance_err: 0.0000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85d52a1411b4eb6a64860a0f2e575cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[END] metrics:\n",
      "rho_bag: 0.8318\n",
      "rho_bag_err: 0.0112\n",
      "auc_instance: 0.9361\n",
      "auc_instance_err: 0.0078\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Simulate data as described in the paper\n",
    "# embeddings, fixed effects, labels, sim bag predictions, sim instance weights\n",
    "# P: number of outputs, simulated from the same embeddings X\n",
    "X, F, Y, u, w = load_data(P=10, seed=0)\n",
    "model = MixMIL.init_with_mean_model(X[\"train\"], F[\"train\"], Y[\"train\"], likelihood=\"binomial\", n_trials=2).to(device)\n",
    "X, F, Y = [{key: val.to(device) for key, val in el.items()} for el in [X, F, Y]]\n",
    "\n",
    "print_metrics(\"[START]\", calc_metrics(model, X, u, w))\n",
    "# Fit model in parallel to each output separately\n",
    "model.train(X[\"train\"], F[\"train\"], Y[\"train\"], n_epochs=2_000)\n",
    "print_metrics(\"[END]\", calc_metrics(model, X, u, w))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
