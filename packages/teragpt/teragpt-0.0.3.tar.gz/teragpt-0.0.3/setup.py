# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['teragpt']

package_data = \
{'': ['*']}

install_requires = \
['einops', 'local-attention', 'torch', 'zetascale']

setup_kwargs = {
    'name': 'teragpt',
    'version': '0.0.3',
    'description': 'Paper - Pytorch',
    'long_description': '[![Multi-Modality](agorabanner.png)](https://discord.gg/qUtxnK2NMf)\n\n# TeraGPT\nZeta present TeraGPT – the simplest implementation for training large language models with tens or hundreds of billions of parameters. This work was inspired by Andrej Karpathy\'s [nanoGPT](https://github.com/karpathy/nanoGPT/tree/master). However, while nanoGPT is designed to train medium sized models up to around the 1B parameter range, TeraGPT leverages the over-powered Zeta framework to use a single simple model definition and training loop to scale to GPT-3 sized models run across zetascale clusters. \n\nAs in nanoGPT, the main training logic is split between [`train.py`](./teragpt/train.py) and [`model.py`](./teragpt/model.py), with a total of 350 lines of simple, readable pytorch code combined. While nanoGPT can replicate GPT-2, gigaGPT is built to be able to replicate something of the scale of GPT-4 (albeit possibly with a dataset upgrade compared to the nanoGPT support). We have tested that models up to 175b parameters in size run functionally correctly at high throughput and have no reason to suspect that you can\'t scale significantly larger.\n\nThe combination of the scale of the hardware, the weight streaming execution mode, and the data parallel scale-out across machines is what provides the magic required for easy scale-out to larger models and larger clusters.\n\n## Install\n`pip3 install teragpt `\n\n\n## Usage\n```python\nimport torch\nfrom teragpt.main import TeraGPT\n\nmodel = TeraGPT(\n    dim=4096,\n    depth=6,\n    heads=8,\n    num_tokens=20000,\n)\n\nx = torch.randint(0, 20000, (1, 4096))\n\nout = model(x)\nprint(out.shape)\n\n```\n\n### Tokenizer\n```python\nfrom teragpt import Tokenizer\n\ntokenizer_name = "hf-internal-testing/llama-tokenizer"\ntokenizer = Tokenizer(tokenizer_name=tokenizer_name)\nencoded_text = tokenizer.encode("This is a sample text")\ndecoded_text = tokenizer.decode(encoded_text)\nprint("Encoded text:", encoded_text)\nprint("Decoded text:", decoded_text)\n\n```\n\n\n### Train\n`trainer.py` sets up the environment for distributed training and then initializes a `Trainer` object to start the training process.\n\n## Environment Variables\n\nThe script uses the following environment variables:\n\n- `MASTER_ADDR`: The address of the master node. This is typically \'localhost\'.\n- `MASTER_PORT`: The port that the master node is listening on. This is typically \'9994\'.\n- `RANK`: The rank of the current node in the distributed training setup. This is typically \'0\' for the master node.\n- `WORLD_SIZE`: The total number of nodes participating in the distributed training. This is typically the number of GPUs available.\n\n## How to Train the Model\n\n1. Set the environment variables `MASTER_ADDR`, `MASTER_PORT`, `RANK`, and `WORLD_SIZE` appropriately for your distributed training setup.\n\n2. Run the script with any additional arguments required by the `Trainer` object.\n\n```bash\npython train.py\n```\n\nPlease note that the exact arguments required by the `Trainer` object will depend on your specific training setup and the model you are training.\n\n## Note\n\nThe comment `[CRITICAL] Pay attention to this when scaling to multiple GPUs and clusters` indicates that the settings for `RANK` and `WORLD_SIZE` are particularly important when scaling the training process to multiple GPUs and clusters. Make sure to set these variables correctly to ensure efficient distributed training.\n\n\n---\n\n## Codebase comparison\nThe standard way to train a GPT-3 sized model is to use frameworks such as Nvidia Megatron. Megatron however is a large and complex framework that’s challenging to implement. This is what motivated the creation of nanoGPT – a light, readable, hackable framework. To quantify the complexity of these frameworks, we counted the lines of code in reach repo. Megatron has 20,507, lines of code while nanoGPT and Teragpt have 639 and 350 lines of code respectively. This supports our primary claim that TeraGPT trains GPT-3 sized models while retaining the simplicity of nanoGPT.\n\nMegatron-LM\n\n| Language                  | files |        blank |      comment |         code|\n| ------------------------- | ----- | ------------ | ------------ | ----------- |\n| Python                    |    99 |         4710 |         4407 |       18395 |\n| C/C++ Header              |     4 |          146 |           90 |        1118 |\n| C++                       |     4 |          137 |          117 |         649 |\n| CUDA                      |     3 |           41 |           20 |         220 |\n| HTML                      |     1 |           15 |            2 |         107 |\n| Bourne Shell              |     1 |            1 |            0 |           9 |\n| make                      |     1 |            2 |            0 |           7 |\n| SUM:                      |   115 |         5052 |         4636 |       20507 |\n\n\nnanoGPT\n\n| Language                  | files |        blank |      comment |         code|\n| ------------------------- | ----- | ------------ | ------------ | ----------- |\n| Python                    |     5 |           90 |          187 |         639 |\n| SUM:                      |     5 |           90 |          187 |         639 |\n\nTeraGPT\n\n| Language                  | files |        blank |      comment |         code|\n| ------------------------- | ----- | ------------ | ------------ | ----------- |\n| Python                    |     3 |          109 |            1 |         350 |\n| SUM:                      |     6 |          109 |            1 |         350 |\n\n\n# License\nApache',
    'author': 'Kye Gomez',
    'author_email': 'kye@apac.ai',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'https://github.com/kyegomez/TeraGPT',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.9,<4.0',
}


setup(**setup_kwargs)
